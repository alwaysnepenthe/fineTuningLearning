# -*- coding: utf-8 -*-
"""learningNote.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rwINFdFLMbMvLONTuQAbcqXRUJvM2X6c
"""

# 微调大模型

##步骤1：划分数据集

# 导入数据集
!pip install --upgrade datasets fsspec
from datasets import load_dataset
from sklearn.model_selection import train_test_split
from datasets import Dataset, DatasetDict

ds = load_dataset("wmt/wmt14", "de-en")

ds_train = ds['train'].select(range(1000)).to_pandas()
#ds_test = ds['test'].to_pandas()


#ds_train_de = ds_train['translation'].apply(lambda x: x['de']).tolist()
#ds_train_en = ds_train['translation'].apply(lambda x: x['en']).tolist()

# 划分ds_train 70%训练 20%测试 10%验证
train, test = train_test_split(ds_train,test_size=0.3,random_state=42)
test, validation = train_test_split(test,test_size=1/3,random_state=42)

dataset = DatasetDict({
    'train': Dataset.from_pandas(train,preserve_index=False),
    'test': Dataset.from_pandas(test,preserve_index=False),
    'validation': Dataset.from_pandas(validation,preserve_index=False)
})

dataset

# 统计列元素出现的次数
label_counts = ds_train['translation'].value_counts()
label_counts

# 获取列特征
label_names = ds['train'].features
label_names

##步骤2： 对数据集进行分词 生成 encode

from transformers import AutoTokenizer,AutoModel
model_checkpoint = "facebook/nllb-200-distilled-600M"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModel.from_pretrained(model_checkpoint)

tokenizer.src_lang = "eng_Latn"
tokenizer.tgt_lang = "deu_Latn"

def tokenize_EnToDe(batch):
  english_sentences = [item['en'] for item in batch['translation']]
  german_sentences = [item['de'] for item in batch['translation']]

  inputs = tokenizer(english_sentences, truncation=True, padding=True)

  # 对德文句子进行分词 (作为模型的标签/目标输出)
  targets = tokenizer(german_sentences, truncation=True, padding=True)

  # 返回一个字典，其中包含分词后的结果，并按照Hugging Face transformers的惯例命名
  # 'input_ids': 源语言句子的 token ids
  # 'attention_mask': 源语言句子的 attention mask
  # 'labels': 目标语言句子的 token ids (作为模型训练的标签)
  # 'labels_attention_mask': 目标语言句子的 attention mask (有时也需要，取决于模型)
  return {
      'input_ids': inputs['input_ids'],
      'attention_mask': inputs['attention_mask'],
      'labels': targets['input_ids'],
      'labels_attention_mask': targets['attention_mask'] # 建议也包含目标语言的 attention mask
  }
# dataset.map将分词后的tokens与id相对应
# batch_size可以设置每个批次导入的数据大小
encoded = dataset.map(tokenize_EnToDe, batched=True,batch_size=None)

encoded

model.config

##步骤3： 配置训练设备
from transformers import AutoConfig, AutoModelForSeq2SeqLM
import torch

# 选取训练设备
if torch.has_mps:
  device = torch.device("mps")
elif torch.cuda.is_available():
  device = torch.device("cuda")
else:
  device = torch.device("cpu")

config = AutoConfig.from_pretrained(model_checkpoint)
config.num_hidden_layers = 6
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint,config=config,force_download=True).to(device)

##步骤4：参数配置

##配置训练参数
from transformers import TrainingArguments
from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score,classification_report
!pip install evaluate
from evaluate import load
#from datasets import load_metric

batch_size = 64
training_dir = "./nllb_finetune_resultss"
training_args = TrainingArguments(
    output_dir=training_dir,
    num_train_epochs=3, #训练周期：遍历样本次数
    learning_rate=2e-5, #学习率 更新模型权重使用的步长
    weight_decay=0.01,  #权重衰减是一种正则化技术，用于防止模型过拟合
    eval_strategy='epoch', #设定评估时间为每个训练epoch结束
    disable_tqdm=False, #启用训练进度条
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,

)

##配置评估参数
def compute_metrics(eval_pred):
    # eval_pred contains predictions and labels
    predictions,labels=eval_pred

    decoded_preds=tokenizer.batch_decode(predictions,skip_special_tokens=True)

    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    decode_labels = [[label] for label in decoded_labels]

    bleu_metric = load("bleu")

    result = bleu_metric.compute(predictions=decoded_preds,references=decode_labels)

    return {"bleu":result["bleu"]}

##步骤5：训练器
from transformers import Trainer
trainer = Trainer(
    model=model,
    args = training_args,
    train_dataset = encoded['train'],
    eval_dataset = encoded['validation'],
    compute_metrics = compute_metrics,
    tokenizer=tokenizer
)
trainer

##步骤6：启动训练器
trainer.train()

##步骤7：保存训练后的模型
trainer.save_model("./nllb_finetune_results_en2de")

##步骤8：使用自己训练的模型
from transformers import pipeline
translator = pipeline("translation",model="./nllb_finetune_results_en2de")
translator("i love NML")